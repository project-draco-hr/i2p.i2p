{
  if (_log.shouldLog(Log.INFO))   _log.info("Starting pusher from " + _inRaw + " to: "+ _out);
  ReusableGZIPInputStream _in=ReusableGZIPInputStream.acquire();
  long written=0;
  ByteArray ba=null;
  try {
    _in.initialize(_inRaw);
    ba=_cache.acquire();
    byte buf[]=ba.getData();
    int read=-1;
    while ((read=_in.read(buf)) != -1) {
      if (_log.shouldLog(Log.DEBUG))       _log.debug("Read " + read + " and writing it to the browser/streams");
      _out.write(buf,0,read);
      _out.flush();
      written+=read;
    }
  }
 catch (  IOException ioe) {
    if (_log.shouldLog(Log.WARN))     _log.warn("Error decompressing: " + written + ", "+ _in.getTotalRead()+ "/"+ _in.getTotalExpanded()+ " from "+ _inRaw+ " to: "+ _out,ioe);
  }
catch (  OutOfMemoryError oom) {
    _log.error("OOM in HTTP Decompressor",oom);
  }
 finally {
    if (_log.shouldInfo())     _log.info("After decompression, written=" + written + " read="+ _in.getTotalRead()+ ", expanded="+ _in.getTotalExpanded()+ ", remaining="+ _in.getRemaining()+ ", finished="+ _in.getFinished()+ " from "+ _inRaw+ " to: "+ _out);
    if (ba != null)     _cache.release(ba);
    if (_out != null)     try {
      _out.close();
    }
 catch (    IOException ioe) {
    }
    try {
      _in.close();
    }
 catch (    IOException ioe) {
    }
  }
  double compressed=_in.getTotalRead();
  double expanded=_in.getTotalExpanded();
  ReusableGZIPInputStream.release(_in);
  if (compressed > 0 && expanded > 0) {
    double ratio=compressed / expanded;
    _context.statManager().addRateData("i2ptunnel.httpCompressionRatio",(int)(100d * ratio));
    _context.statManager().addRateData("i2ptunnel.httpCompressed",(long)compressed);
    _context.statManager().addRateData("i2ptunnel.httpExpanded",(long)expanded);
  }
}
